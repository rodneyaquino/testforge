# ğŸ”§ TestForge â€” Full-Stack QA Framework

A practical, full-stack QA toolkit that helps teams validate Web UI, APIs, performance signals, and agent behavior with confidence.  
It establishes reliable workflows and surfaces insights you can use to make informed decisions fast.

---

## ğŸ“‹ Pre-Requisites

**Developed and tested on macOS**. For the smoothest setup experience on Mac, we recommend having [Homebrew](https://brew.sh/) installed.

- **Node.js 18+**: Required for Playwright and web test execution.  
- **Python 3.10+**: Required for API tests and agent evaluation scripts.  
- **k6** (optional): Required for performance testing; if missing, performance tests will be skipped gracefully.  
- **Homebrew** (macOS): Recommended package manager for installing Node.js, Python, and k6 on macOS.  
- Internet access to public demo APIs and services for testing.

---

## ğŸš€ Quick Start
```bash
git clone https://github.com/rodneyaquino/testforge.git
cd testforge
make setup
make test-all
make report
```

- `make setup` â€” Installs dependencies and creates necessary output folders.  
- `make test-all` â€” Runs all test suites: Smoke, Web UI, API, and Performance.  
- `make open-report` â€” Opens the Playwright HTML test report (macOS only).  
- `make validate` â€” Runs release readiness gates and enforces quality checks.

---

## ğŸ§© What It Covers

| Layer          | Tooling                     | Focus                                                        |
|----------------|-----------------------------|--------------------------------------------------------------|
| ğŸŒ **Web UI**   | Playwright                  | Login flows, cart and checkout, sorting, two negative scenarios, accessibility scan with axe-core. |
| ğŸ”Œ **Public APIs** | pytest + requests + jsonschema | Public demo APIs covering auth, CRUD, pagination, headers, schema contracts, idempotent operations. |
| âš¡ **Performance** | k6                         | Low-intensity micro-benchmark (1 Virtual User, ~1 RPS, 60s) designed to respect public API load limits. |
| ğŸ¤– **Agentic AI** | Manual prompts + rubric     | Evaluations on JSON schema fidelity, citation correctness, safety refusals, instruction and distraction handling, multi-turn revisions. |
| ğŸ§­ **Governance** | Strategy Â· Matrix Â· Bugs    | Traceable quality artifacts from requirements through release gates, with bug reports linked to tests. |
| ğŸ”„ **CI/CD** | GitHub Actions | Automated test execution and quality gate enforcement on every push and pull request. |

---

## ğŸ—‚ Structure
```
testforge/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                                # GitHub Actions CI/CD workflow
â”œâ”€â”€ .gitignore                                    # Git ignore patterns for dependencies and artifacts
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json                             # VS Code workspace settings
â”œâ”€â”€ Makefile                                      # Orchestration entrypoint for setup, tests, and gates
â”œâ”€â”€ README.md                                     # This file: project overview and setup guide
â”œâ”€â”€ agent-evals/                                  # Agent evaluation suite with prompts and rubric
â”‚   â”œâ”€â”€ README.md                                 # Agent evaluation overview and instructions
â”‚   â”œâ”€â”€ prompts.md                                # Agent test prompts for reasoning, safety, and citations
â”‚   â”œâ”€â”€ results-template.md                       # Template for recording agent evaluation results
â”‚   â”œâ”€â”€ results.md                                # Completed agent evaluation results
â”‚   â”œâ”€â”€ rubric.md                                 # Pass/fail grading rubric for agent responses
â”‚   â”œâ”€â”€ transcripts/                              # Agent test transcripts with evidence
â”‚   â”‚   â””â”€â”€ evidence/                             # Screenshots and evidence for agent tests
â”‚   â””â”€â”€ TEMPLATE.md                               # Template for creating new agent transcripts
â”œâ”€â”€ reports/                                      # Bug reports and quality evidence
â”‚   â””â”€â”€ bugs/                                     # Bug tracking with detailed reports
â”‚       â”œâ”€â”€ BUG_TEMPLATE.md                       # Template for filing new bug reports
â”‚       â””â”€â”€ evidence/                             # Bug evidence artifacts
â”œâ”€â”€ strategy/                                     # Strategic planning and traceability
â”‚   â”œâ”€â”€ TEST_STRATEGY.md                          # Comprehensive test strategy document
â”‚   â””â”€â”€ TRACEABILITY_MATRIX.md                    # Mapping of requirements to test coverage
â””â”€â”€ tests/                                        # All test suites organized by layer
    â”œâ”€â”€ api/                                      # API test suite (pytest)
    â”‚   â”œâ”€â”€ conftest.py                           # pytest fixtures and shared configuration
    â”‚   â”œâ”€â”€ requirements.txt                      # Python dependencies for API tests
    â”‚   â””â”€â”€ test_public_apis.py                   # API contract and behavior tests
    â”œâ”€â”€ integration/                              # Integration and smoke tests
    â”‚   â””â”€â”€ smoke_integration.py                  # Reachability and smoke checks for services
    â”œâ”€â”€ perf/                                     # Performance testing suite (k6)
    â”‚   â”œâ”€â”€ baseline_perf.js                      # k6 micro-benchmark script
    â”‚   â”œâ”€â”€ perf_gate.py                          # Performance gate checker enforcing SLOs
    â”‚   â”œâ”€â”€ PERF_PLAN.md                          # Performance testing plan and rationale
    â”‚   â””â”€â”€ results/                              # Performance test results (generated)
    â”‚       â””â”€â”€ k6-summary.json                   # k6 test summary with metrics
    â””â”€â”€ web/                                      # Web UI test suite (Playwright)
        â”œâ”€â”€ a11y.spec.ts                          # Accessibility tests using axe-core
        â”œâ”€â”€ cart.spec.ts                          # Cart and sorting behavior tests
        â”œâ”€â”€ checkout_negative.spec.ts             # Negative checkout validation tests
        â”œâ”€â”€ login.spec.ts                         # Login flow tests (valid and invalid)
        â”œâ”€â”€ package.json                          # Node dependencies for Playwright tests
        â””â”€â”€ playwright.config.ts                  # Playwright configuration settings
```

---

## ğŸ§ª Gates (Release Readiness)

- âœ… All critical Web UI flows must pass.  
- âœ… Zero critical accessibility (a11y) violations on inventory page.  
- âœ… API contracts and schema validations pass without regressions.  
- âœ… Agent evaluation manual scoring meets or exceeds 80% pass rate.  
- âœ… Performance p95 latency below 2000 ms and error rate below 2%.  
- âœ… No open Critical bug reports blocking release.

These gates are automatically validated via:
```bash
make validate
```

**CI/CD automation:** The GitHub Actions workflow (`.github/workflows/ci.yml`) runs these gates automatically on every push to `main` or `develop` branches and on all pull requests.

---

## ğŸ§­ Notes

- Web tests run in isolated browser contexts; setup and teardown are automatic and stateless.  
- API targets are public mocks designed for idempotency and statelessness; POSTs echo data, DELETes verify idempotency.  
- Performance tests are read-only and designed to be low-impact on public services.  
- Agent evaluations are recorded as transcripts and screenshots for manual review using a detailed rubric.
- CI/CD pipeline uploads test results and Playwright reports as artifacts for easy review.

---

## â± Time Spent and Assumptions

**Estimated Time (~10 hours)**  
- 1.0 hour: Planning and strategy formulation  
- 2.0 hours: Playwright flows and accessibility testing  
- 2.5 hours: API suite development, including schema validations  
- 1.0 hour: Integration smoke test implementation  
- 1.0 hour: k6 micro-benchmark development and performance plan creation  
- 1.0 hour: Agent prompt design, rubric creation, and transcript collection  
- 1.5 hours: Documentation, traceability matrix, and CI pipeline gates setup

**Assumptions**  
- Public demo targets remain stable during testing window.  
- No private secrets or paid credentials are required.  
- All tests are idempotent and self-contained for reproducibility.  
- Agent evaluations use human-reviewed transcript evidence for scoring.

---

## ğŸ§© Helpful Commands

| Command          | Description                              |
|------------------|------------------------------------------|
| `make setup`     | One-time install of all dependencies and setup folders |
| `make test-all`  | Runs all test suites: Smoke, Web, API, and Perf  |
| `make report`    | Prints out report locations and artifact summaries |
| `make open-report` | Opens the Playwright HTML report (macOS only)  |
| `make validate`  | Runs all quality gates and enforces release criteria |
| `make clean`     | Cleans test results and temporary files |
| `make clean-all` | Deep cleans including node modules and Python packages |

---

## ğŸ“˜ Included Artifacts

- `strategy/TEST_STRATEGY.md` â€” In-depth scope, risks, data management, and release gates  
- `strategy/TRACEABILITY_MATRIX.md` â€” Detailed mapping of test coverage to requirements  
- `agent-evals/prompts.md` â€” Prompt suite covering reasoning, safety checks, and citation fidelity  
- `agent-evals/rubric.md` â€” Pass/fail grading rubric for agent evaluations  
- `agent-evals/results-template.md` â€” Template for agent test result documentation  
- `tests/perf/PERF_PLAN.md` â€” Micro-benchmark design and resilience planning  
- `reports/bugs/*.md` â€” Bug reports with detailed evidence  
- `.github/workflows/ci.yml` â€” GitHub Actions CI/CD workflow automating test execution
- `Makefile` â€” Single orchestration command entrypoint

---

## ğŸ¯ Purpose

TestForge is a simple, easy-to-learn QA framework that demonstrates how to test a complete software stack from end to end. It covers testing the web user interface, public APIs, performance, and AI agent behavior. The goal is to teach how to build clear, reliable tests that provide trustworthy evidence and readable reports across all these layers.

![make-validate-sample](sample.png)